{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430e7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7695c",
   "metadata": {},
   "source": [
    "## String Evaluators\n",
    "\n",
    "### 1. Criteria Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50df8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5c7bc",
   "metadata": {},
   "source": [
    "List of criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e079fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(Criteria))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835412c0",
   "metadata": {},
   "source": [
    "#### a. Conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f4ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee394ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b487d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion for this task is conciseness. The question asked is \"Who is the president of United States?\" The answer provided does correctly identify the current president of the United States as Joe Biden. However, the submission goes beyond simply identifying the president and provides additional information about Biden\\'s life, education, and political career. While this information is accurate and relevant to Biden, it is not necessary to answer the question asked. Therefore, the submission is not concise and to the point, and does not meet the criterion.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"Joe Biden is an American politician who is the 46th and current president of the United States. Born in Scranton, Pennsylvania on November 20, 1942, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972.\",\n",
    "    input=\"Who is the president of United States?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8f03c",
   "metadata": {},
   "source": [
    "#### b. Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6120958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"The criterion for this task is the correctness of the submission. This means the submitted answer should be accurate, factual, and correct in response to the given input.\\n\\nThe input asks if there is any river on the moon. The submitted answer states that there is no evidence of a river on the Moon. \\n\\nThe reference information talks about a hypothetical future where lunar scientists discovered a subterranean river beneath the Moon's surface. However, this is a hypothetical scenario and not a factual statement. \\n\\nTherefore, the submission is correct as it aligns with the current known facts about the Moon. The hypothetical scenario in the reference does not invalidate the correctness of the submission.\\n\\nSo, the submission meets the criteria.\\n\\nY\", 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"Is there any river on the moon?\",\n",
    "    prediction=\"There is no evidence of river on the Moon\",\n",
    "    reference=\"In a hypothetical future, lunar scientists discovered an astonishing phenomenonâ€”a subterranean river beneath the Moon's surface\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7050d0",
   "metadata": {},
   "source": [
    "#### c. Custom Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b70cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6199240",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_criteria = {\n",
    "    \"numeric\": \"Does the output contain numeric information?\",\n",
    "    \"mathematical\": \"Does the output contain mathematical information?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "496f78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a joke\"\n",
    "\n",
    "output = \"\"\"\n",
    "Why did the mathematician break up with his girlfriend?\n",
    "\n",
    "Because she had too many \"irrational\" issues!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "782bdf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Multi-criteria evaluation =====================\n",
      "{'reasoning': 'First, let\\'s assess the first criterion: Does the output contain numeric information? The joke does not contain any numbers, percentages, or any other form of numeric data. Therefore, the submission does not meet the first criterion.\\n\\nNext, let\\'s assess the second criterion: Does the output contain mathematical information? The joke does mention a mathematician and the term \"irrational\" which is a mathematical term. Therefore, the submission does meet the second criterion.\\n\\nHowever, for the submission to meet all criteria, it must satisfy both. Since it does not meet the first criterion, the submission does not meet all the criteria.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    ")\n",
    "eval_result = eval_chain.evaluate_strings(prediction = output, input = prompt)\n",
    "print(\"===================== Multi-criteria evaluation =====================\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e89f9",
   "metadata": {},
   "source": [
    "### 2. Embedding Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1dc3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"embedding_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_strings(prediction=\"Total Profit is 04.25 Cr\", reference=\"Total return is 4.25 Cr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1eba25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.06977808876949698}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_strings(prediction=\"Total Profit is 04.25 Cr\", reference=\"Total return is 4.25 Cr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f9bc0",
   "metadata": {},
   "source": [
    "**List of Distance Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d44fc830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<EmbeddingDistance.COSINE: 'cosine'>,\n",
       " <EmbeddingDistance.EUCLIDEAN: 'euclidean'>,\n",
       " <EmbeddingDistance.MANHATTAN: 'manhattan'>,\n",
       " <EmbeddingDistance.CHEBYSHEV: 'chebyshev'>,\n",
       " <EmbeddingDistance.HAMMING: 'hamming'>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import EmbeddingDistance\n",
    "list(EmbeddingDistance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455287a9",
   "metadata": {},
   "source": [
    "### 3. Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec4ff4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import ExactMatchStringEvaluator\n",
    "exact_match_evaluator = ExactMatchStringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd029714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match_evaluator.evaluate_strings(\n",
    "    prediction=\"Data Science\",\n",
    "    reference=\"Data science\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcee56b",
   "metadata": {},
   "source": [
    "### 4. Evaluating Structured Output: JSON Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2974d",
   "metadata": {},
   "source": [
    "#### a. JsonValidityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70f193f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import JsonValidityEvaluator\n",
    "\n",
    "evaluator = JsonValidityEvaluator()\n",
    "\n",
    "prediction = '{\"name\": \"Hari\", \"Exp\": 5, \"Location\": \"Pune\"}'\n",
    "\n",
    "result = evaluator.evaluate_strings(prediction=prediction)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eadee7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 47 (char 46)'}\n"
     ]
    }
   ],
   "source": [
    "prediction = '{\"name\": \"Hari\", \"Exp\": 5, \"Location\": \"Pune\",}'\n",
    "result = evaluator.evaluate_strings(prediction=prediction)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025bd5b",
   "metadata": {},
   "source": [
    "#### b. JsonEqualityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "832ff9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': False}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import JsonEqualityEvaluator\n",
    "\n",
    "evaluator = JsonEqualityEvaluator()\n",
    "result = evaluator.evaluate_strings(prediction='{\"Exp\": 2}', reference='{\"Exp\": 2.5}')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5480d0",
   "metadata": {},
   "source": [
    "#### c. JsonEditDistanceEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8622d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.043478260869565216}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import JsonEditDistanceEvaluator\n",
    "\n",
    "evaluator = JsonEditDistanceEvaluator()\n",
    "\n",
    "\n",
    "result = evaluator.evaluate_strings(\n",
    "    prediction='{\"name\": \"Ram\", \"Exp\": 2}', reference='{\"name\": \"Rama\", \"Exp\": 2}'\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf54e6e",
   "metadata": {},
   "source": [
    "#### d. JsonSchemaEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13439682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import JsonSchemaEvaluator\n",
    "\n",
    "evaluator = JsonSchemaEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98daa53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': False, 'reasoning': \"<ValidationError: '4 is less than the minimum of 5'>\"}\n"
     ]
    }
   ],
   "source": [
    "result = evaluator.evaluate_strings(\n",
    "    prediction='{\"name\": \"Rama\", \"Exp\": 4}',\n",
    "    reference='{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"},'\n",
    "    '\"Exp\": {\"type\": \"integer\", \"minimum\": 5}}}',\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726b536",
   "metadata": {},
   "source": [
    "### 5. Regex Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c0df6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import RegexMatchStringEvaluator\n",
    "evaluator = RegexMatchStringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "227242e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_strings(\n",
    "    prediction=\"Joining date is 2021-04-26\",\n",
    "    reference=\".*\\\\b\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\b.*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5a8c5",
   "metadata": {},
   "source": [
    "### 6. Scoring Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3b577af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6a48ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "873a2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\n",
    "    \"labeled_score_string\",\n",
    "    criteria=accuracy_criteria,\n",
    "    llm=ChatOpenAI(model=\"gpt-4\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e17235e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"The assistant's response is accurate and correctly tells the user where their socks are located - in the third drawer of the dresser. The phrasing is slightly different from the reference but the meaning and content are the same. Therefore, the response deserves a high score. Rating: [[10]].\", 'score': 10}\n"
     ]
    }
   ],
   "source": [
    "# Correct\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"You can find them in the dresser's third drawer.\",\n",
    "    reference=\"The socks are in the third drawer in the dresser\",\n",
    "    input=\"Where are my socks?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2893add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"The AI's response is partially correct as it does mention the dresser which is included in the reference answer. However, it lacks the specific detail of the socks being in the third drawer of the dresser. Its answer is therefore incomplete and not as accurate as it could be. Rating: [[7]]\", 'score': 7}\n"
     ]
    }
   ],
   "source": [
    "# Correct but lacking information\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"You can find them in the dresser.\",\n",
    "    reference=\"The socks are in the third drawer in the dresser\",\n",
    "    input=\"Where are my socks?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ca85974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"The assistant's response does not align with the ground truth at all. According to the ground truth, the socks are in the third drawer in the dresser, not in the dog's bed as suggested by the assistant. Therefore, the response is completely inaccurate. Rating: [[1]].\", 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "# Incorrect\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"You can find them in the dog's bed.\",\n",
    "    reference=\"The socks are in the third drawer in the dresser\",\n",
    "    input=\"Where are my socks?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929a64d",
   "metadata": {},
   "source": [
    "### 7. String Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2096cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(\"string_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407bfc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.23015873015873023}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_strings(\n",
    "    prediction=\"Senior Data Scientist\",\n",
    "    reference=\"Data Scientist\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e254f",
   "metadata": {},
   "source": [
    "## II. Comparison Evaluators\n",
    "\n",
    "### 1. Pairwise string comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84ba210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "338af72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"Assistant A's response is incorrect. There are not 5 days in a week. Assistant B's response, while brief, is accurate. There are indeed 7 days in a week. Therefore, Assistant B's response is more helpful, relevant, correct, and factual. The depth of thought is not applicable in this case as the question is straightforward and factual. \\n\\nFinal verdict: [[B]]\",\n",
       " 'value': 'B',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"there are 5 days\",\n",
    "    prediction_b=\"7\",\n",
    "    input=\"how many days in a week?\",\n",
    "    reference=\"Seven\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ba609",
   "metadata": {},
   "source": [
    "### 2. Pairwise embedding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54c3b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"pairwise_embedding_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13f08692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.02634930736279606}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Rajasthan is hot in June\", prediction_b=\"Rajasthan is warm in June.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python langchain_env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
